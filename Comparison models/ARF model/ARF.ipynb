{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# <FONT COLOR='red'>**_OVERALL DESCRIPTION_**</FONT>\n",
        "\n",
        "---\n",
        "---\n",
        "\n",
        "The purpose of this notebook is to replicate, with current libraries, the construction of the model with the ARF (Adaptive Random Forest) algorithm implemented by NELLY so that it can correctly classify elephant and mouse flows from DCN traffic traces using a set of modified UNI1 data. called UNIV1."
      ],
      "metadata": {
        "id": "aEickIGxpXai"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <FONT COLOR = 'gray'>**INSTALL LIBRARIES**</FONT>\n",
        "\n",
        "---\n",
        "\n",
        "Next, we need install some libraries to use the model.\n",
        "\n",
        "1. `pip install river`: the command allows you to install the `river` library of 2022, the `river` library is the result of the merger between the `creme` (Halfordet al. 2019) and `scikit-multiflow` libraries (Montiel et al. 2018)."
      ],
      "metadata": {
        "id": "Jj0gncXNp5N4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install river"
      ],
      "metadata": {
        "id": "FXcmMOJI2Ju6"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <FONT COLOR = 'gray'>**IMPORT MODEL LIBRARIES**</FONT>\n",
        "\n",
        "---\n",
        "\n",
        "Next, we need import some libraries to use the model.\n",
        "\n",
        "1. `from river import evaluate, forest, metrics, preprocessing, stream`: `evaluate` is imported, a module that provides tools for the evaluation of `ML` models such as `ARF` where models are evaluated as they are updated with new data. One of the most common functions is `evaluate.progressive_val_score`, which allows you to evaluate the performance of a model progressively. The `forest` module contains implementations of random forest algorithms adapted for streaming learning. forest includes models such as Adaptive Random Forest, which is a variant of the random forest designed to adapt to changes in data over time (conceptual drifts). The `metrics` library provides access to metrics such as `accuracy`, `precision`, `recall` and `f1` designed to be updated continuously.\n",
        "\n",
        "  On the other hand, there is the `preprocessing` library, which is a module that contains tools for preprocessing data in continuous streams. Finally you have `stream` which provides classes and functions to handle data streams. It includes synthetic data generators, streaming data file readers, and tools to simulate the arrival of sequential data.\n",
        "\n",
        "## <FONT COLOR = 'gray'>**IMPORT DATA ANALYSIS LIBRARIES**</FONT>\n",
        "\n",
        "---\n",
        "\n",
        "Next, we need import some libraries to data analysis graphics.\n",
        "5. `import pandas as pd`: An essential component for data analysis and manipulation in Python, it provides data structures such as Series (one-dimensional) and DataFrames (two-dimensional) that allow you to handle tabular data with ease.\n",
        "6. `import numpy as np`:Fundamental component for numerical computation in Python, as it provides support for multidimensional arrays and matrices, as well as a large collection of mathematical functions to operate on these arrays."
      ],
      "metadata": {
        "id": "dlcMVcfRqWoC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# IMPORT MODEL LIBRARIES\n",
        "from river import evaluate, forest, metrics, preprocessing, stream\n",
        "\n",
        "# IMPORT DATA ANALYSIS LIBRARIES\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "wPa1eDO525rI"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <FONT COLOR = 'orange'>**LOAD DATASET**</FONT>\n",
        "\n",
        "---\n",
        "\n",
        "We will use the UNIV1 dataset which is a modification of the UNI1 dataset, which contains traffic traces from several DCNs collected by a university in 2010. The traffic traces in the dataset are only IPv4.\n",
        "\n",
        "UNIV1 contain the follow structure:\n",
        "\n",
        "1. `start_time`: Represent the beginning of the capture per flow.\n",
        "2. `end_time`: Represent the finish of the capture per flow.\n",
        "3. `ip_src`: Represent the source IPv4.\n",
        "4. `ip_dst`: Represent the destination IPv4.\n",
        "5. `ip_proto`: Represent the IP protocol used.\n",
        "6. `port_src`: Represent the source port.\n",
        "7. `port_dst`: Represent the destination port.\n",
        "8. `size_pkt1`: Represent the size of the packet 1.\n",
        "9. `size_pkt2`: Represent the size of the packet 2.\n",
        "10. `size_pkt3`: Represent the size of the packet 3.\n",
        "11. `size_pkt4`: Represent the size of the packet 4.\n",
        "12. `size_pkt5`: Represent the size of the packet 5.\n",
        "13. `size_pkt6`: Represent the size of the packet 6.\n",
        "14. `size_pkt7`: Represent the size of the packet 7.\n",
        "15. `iat_pkt2`: Represent the inter arrive time of the packet 2.\n",
        "16. `iat_pkt3`: Represent the inter arrive time of the packet 3.\n",
        "17. `iat_pkt4`: Represent the inter arrive time of the packet 4.\n",
        "18. `iat_pkt5`: Represent the inter arrive time of the packet 5.\n",
        "19. `iat_pkt6`: Represent the inter arrive time of the packet 6.\n",
        "20. `iat_pkt7`: Represent the inter arrive time of the packet 7.\n",
        "21. `tot_size`: Represent the total size per flow.\n",
        "22. `flow_type`: Represent the flow type as elephant or mice."
      ],
      "metadata": {
        "id": "I2PWj74asi6x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# UNIV1 DATASET ID\n",
        "id = '1Nt4A7U0P_2x7VYfX0T7Tr21tt3CHIREl'\n",
        "\n",
        "# GENERATE THE DOWNLOAD URL\n",
        "url_univ1 = f'https://drive.google.com/uc?id={id}'\n",
        "\n",
        "# DOWNLOAD AND LOAD UNI1 DATASET IN A DATAFRAME OF PANDAS\n",
        "univ1_df = pd.read_csv(url_univ1)\n",
        "univ1_df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "29ia76ito_Nz",
        "outputId": "7b08c00a-f23c-4129-d378-641a9dfa6109"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 73256 entries, 0 to 73255\n",
            "Data columns (total 22 columns):\n",
            " #   Column      Non-Null Count  Dtype \n",
            "---  ------      --------------  ----- \n",
            " 0   start_time  73256 non-null  int64 \n",
            " 1   end_time    73256 non-null  int64 \n",
            " 2   ip_src      73256 non-null  int64 \n",
            " 3   ip_dst      73256 non-null  int64 \n",
            " 4   ip_proto    73256 non-null  int64 \n",
            " 5   port_src    73256 non-null  int64 \n",
            " 6   port_dst    73256 non-null  int64 \n",
            " 7   size_pkt1   73256 non-null  int64 \n",
            " 8   size_pkt2   73256 non-null  int64 \n",
            " 9   size_pkt3   73256 non-null  int64 \n",
            " 10  size_pkt4   73256 non-null  int64 \n",
            " 11  size_pkt5   73256 non-null  int64 \n",
            " 12  size_pkt6   73256 non-null  int64 \n",
            " 13  size_pkt7   73256 non-null  int64 \n",
            " 14  iat_pkt2    73256 non-null  int64 \n",
            " 15  iat_pkt3    73256 non-null  int64 \n",
            " 16  iat_pkt4    73256 non-null  int64 \n",
            " 17  iat_pkt5    73256 non-null  int64 \n",
            " 18  iat_pkt6    73256 non-null  int64 \n",
            " 19  iat_pkt7    73256 non-null  int64 \n",
            " 20  tot_size    73256 non-null  int64 \n",
            " 21  flow_type   73256 non-null  object\n",
            "dtypes: int64(21), object(1)\n",
            "memory usage: 12.3+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <FONT COLOR = 'orange'>**DATA CLEANING**</FONT>\n",
        "\n",
        "---\n",
        "\n",
        "For the creation of any type of ML (Machine Learning) model, it is necessary to perform data cleaning to, in this case, manipulate the records so that their data type is interpretable by the `ARF` algorithm, as well as, perform the replacing elephant with 1 and mice with 0 in the target prediction field."
      ],
      "metadata": {
        "id": "fbesDatnssAO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CONVERT UNIQUE FLOW_TYPE VALUES THAT ARE OBJECT TO NUMERIC\n",
        "print(f'Unique values of flow_type: {univ1_df[\"flow_type\"].unique()}')\n",
        "print(f'mice: 0\\nelephant: 1\\n')\n",
        "univ1_df['flow_type'] = univ1_df['flow_type'].map({'mice': 0, 'elephant': 1})\n",
        "print(f'New unique values of flow_type: {univ1_df[\"flow_type\"].unique()}\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rwyZHT20v1-z",
        "outputId": "e323520b-fd37-443d-ad7b-5beb642bf76b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique values of flow_type: ['mice' 'elephant']\n",
            "mice: 0\n",
            "elephant: 1\n",
            "\n",
            "New unique values of flow_type: [0 1]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CONVERT UNIV1 INTO A RIVER DATA STREAM\n",
        "dataset = stream.iter_pandas(\n",
        "    X=univ1_df.drop(columns=['flow_type']),\n",
        "    y=univ1_df['flow_type']\n",
        ")"
      ],
      "metadata": {
        "id": "6kUeuVIKvluq"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# IMPLEMENTATION OF A STANDARDIZER\n",
        "scaler = preprocessing.StandardScaler()"
      ],
      "metadata": {
        "id": "RoU92zlewLAm"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CREATE ARF MODEL\n",
        "arf_model = forest.ARFClassifier(\n",
        "    n_models=5,\n",
        "    max_depth=5,\n",
        "    max_features='log2',\n",
        "    lambda_value=3,\n",
        "    min_branch_fraction=0.15,\n",
        "    max_share_to_split=0.85,\n",
        "    split_criterion='gini',\n",
        "    leaf_prediction='nb',\n",
        "    merit_preprune=True,\n",
        "    delta=0.25,\n",
        "    seed=42\n",
        ")"
      ],
      "metadata": {
        "id": "4r8N21rIpjMh"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CHAIN ​​PREPROCESSING AND MODEL\n",
        "model = scaler | arf_model"
      ],
      "metadata": {
        "id": "i6l-hNQys1vz"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DEFINE THE METRICS\n",
        "metricas = metrics.ClassificationReport()"
      ],
      "metadata": {
        "id": "3NmFIiiRwgu5"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PERFORM PROGRESSIVE VALIDATION AND CALCULATE METRICS\n",
        "evaluate.progressive_val_score(dataset, model, metric=metricas)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H36NyGZRzuYY",
        "outputId": "60365c66-aad0-41d1-ad76-1835db609c8a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           Precision   Recall   F1       Support  \n",
              "                                                  \n",
              "       0      98.75%   94.79%   96.73%     47531  \n",
              "       1      91.04%   97.78%   94.29%     25724  \n",
              "                                                  \n",
              "   Macro      94.89%   96.29%   95.51%            \n",
              "   Micro      95.84%   95.84%   95.84%            \n",
              "Weighted      96.04%   95.84%   95.87%            \n",
              "\n",
              "                 95.84% accuracy                  "
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    }
  ]
}